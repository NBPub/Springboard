{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4640145-effd-4374-82f3-4755951fa37d",
   "metadata": {},
   "source": [
    "- Contents\n",
    "  - 16.1, 16.2 Feature Engineering (C1-C4)\n",
    "    - Creating Features *no notes*\n",
    "    - [Text Data](#Text-Data)\n",
    "    - [Cleaning Data](#Data-Cleaning)\n",
    "    - [Statistical Analysis](#Data-Distributions)\n",
    "  - 16.1.5 Image Processing *optional* (C1)\n",
    "    - [Image Processing Intro, scikit-image](#Image-Data)\n",
    "  - 16.2.2 Imputation Techniques *optional* (C3,C4)\n",
    "    - [Dealing with Missing Data](#Data-Imputation)\n",
    "  - 16.2.5 `featuretools` Automated Feature Engineering\n",
    "    - [FeatureTools package](#featuretools)\n",
    "    - *notebook saved in separate folder*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f42322-ef08-43f0-bb00-060bccdec24e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032bd18-0994-44d9-972a-d12278e0f9de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Text Data\n",
    " - pandas string operations\n",
    " - `sklearn.feature_extraction.text` \n",
    "   - CountVectorizer [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "   - TF-IDF Vectorizer [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "   - Bag of words, N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83afac8-f9da-455d-aaee-aae88aa2a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "print(speech_df['text_clean'].head())\n",
    "\n",
    "# Find the length of each text, they did not remove whitespace\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.replace(' ','').str.len()\n",
    "\n",
    "# Count the number of words in each text, they did not like splitting by space\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split(' ').str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef75ef-9af7-4d6d-a235-64fbe42bb995",
   "metadata": {},
   "source": [
    "**Count Vectorizer scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f382d-61d9-47fb-9e2c-d4e6fd624c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "print(cv.get_feature_names_out())\n",
    "# Print feature names\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array.shape)\n",
    "\n",
    "# Specify arguements to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "cv_df = pd.DataFrame(cv_array, \n",
    "                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d3ad-51e0-47f9-87a7-31259d80801b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68f7e0-9b95-4f30-8d7f-b089c19b4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer, 8768 cols without setting max_features\n",
    "tv = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
    "                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())\n",
    "\n",
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0,:]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e99a8-ebda-4940-b95d-f1778a40ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
    "                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342efbe-da7a-49d7-ab2b-2d61cbd6e03a",
   "metadata": {},
   "source": [
    "**N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a657d32-4be3-411c-8983-4a1eb1c902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(max_features=100, \n",
    "                                 stop_words='english', \n",
    "                                 ngram_range=(3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "print(cv_trigram_vec.get_feature_names())\n",
    "\n",
    "# Create a DataFrame of the features\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \n",
    "                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "print(cv_tri_df.sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a2aee-ce6f-41c0-a6e3-f6a6df834796",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Data Cleaning\n",
    "- pandas operations for detecting/counting/filling Null\n",
    "- \"Bad Data\"\n",
    "  - strings to numbers\n",
    "  - use `.apply()` for custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9275ed-000a-4e52-a7b0-59394cf6c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values of column with the mean\n",
    "df['col'].fillna(df['col'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a740cd-eae2-4ea7-b01f-2b3768d1008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use method chaining\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "                              .str.replace(',', '')\\\n",
    "                              .str.replace('$','')\\\n",
    "                              .str.replace('£','')\\\n",
    "                              .astype(float)\n",
    " \n",
    "# Print the RawSalary column\n",
    "print(so_survey_df['RawSalary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2207dd-3fd4-4237-82f7-16da82ebb5d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Data Distributions\n",
    " - pandas df to graph operations\n",
    " - matplotlib + seaborn plots\n",
    " - scaling and transforming data\n",
    "   - not always useful for decision tree models\n",
    " - outlier removal\n",
    " - train / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948786c-9627-4039-a805-b001bfe95935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "so_numeric_df.hist()\n",
    "plt.show()\n",
    "\n",
    "# Create a boxplot of two columns\n",
    "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "# Create a boxplot of ConvertedSalary\n",
    "so_numeric_df.ConvertedSalary.plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc7199-caa0-411c-a56c-55aef006132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50644652-5269-45ec-a6e1-b922ccf6e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships\n",
    "sns.pairplot(so_numeric_df)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098eebf-b970-4c26-a457-100b1fbe2748",
   "metadata": {},
   "source": [
    "**Scalers, Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab80a2-431c-42c4-9fbb-4d02735529a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MinMaxScaler, repeat exercsie with Standard\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "MM_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit MM_scaler to the data\n",
    "MM_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "print(so_numeric_df[['Age_MM', 'Age']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67950bef-ff94-4a8c-9179-c17871ea34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Instantiate PowerTransformer\n",
    "pow_trans = PowerTransformer()\n",
    "\n",
    "# Train the transform on the data\n",
    "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Apply the power transform to the data\n",
    "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Plot the data before and after the transformation\n",
    "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6d035-9f79-42d5-9959-120e63368acb",
   "metadata": {},
   "source": [
    "**Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39077607-0597-49c7-8da5-8aac95848940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Based Removal\n",
    "# Find the 95th quantile\n",
    "quantile = so_numeric_df['ConvertedSalary'].quantile(q=0.95)\n",
    "\n",
    "# Trim the outliers\n",
    "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "\n",
    "# The original histogram\n",
    "so_numeric_df[['ConvertedSalary']].hist()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# The trimmed histogram\n",
    "trimmed_df[['ConvertedSalary']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbceb5-095a-4428-aeab-7cb2d8ceda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Removal ± SD\n",
    "# Find the mean and standard dev\n",
    "std = so_numeric_df['ConvertedSalary'].std()\n",
    "mean = so_numeric_df['ConvertedSalary'].mean()\n",
    "\n",
    "# Calculate the cutoff\n",
    "cut_off = std * 3\n",
    "lower, upper = mean - cut_off, mean+cut_off\n",
    "\n",
    "# Trim the outliers\n",
    "trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper) \\\n",
    "                           & (so_numeric_df['ConvertedSalary'] > lower)]\n",
    "\n",
    "# The trimmed box plot\n",
    "trimmed_df[['ConvertedSalary']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c0176-61b4-4c9e-85b5-1d2a6913ea9b",
   "metadata": {},
   "source": [
    "**Train/Test principles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ac899-8c65-4294-babe-7be7169d529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply a standard scaler to the data\n",
    "SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit the standard scaler to the data\n",
    "SS_scaler.fit(so_train_numeric[['Age']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "print(so_test_numeric[['Age', 'Age_ss']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af139be3-4059-4d13-95bf-d32bca27979e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f32736-626d-4fc6-bf49-0636cc4ee9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee780803-0c9c-4799-a5ec-a1a380f81026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dac8e3-98d1-4f3c-bcf8-3ce44421de08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc8bb5e-75a2-4a4b-ba7a-9d2c6f7788b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Imputation\n",
    " - *simple imputer, plot results*\n",
    " - pd `fillna`, [`interpolate`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html)\n",
    " - **fancyimpute** package\n",
    "   - [github](https://github.com/iskandr/fancyimpute)\n",
    "   - [slides](https://s3.amazonaws.com/assets.datacamp.com/production/course_17404/slides/chapter4.pdf)\n",
    "   - *use **sklearn** for IterativeImputer, not **fancyimpute***\n",
    "- sklearn [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) for categorical data\n",
    "- imputation comparison, using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e31aa-eacc-4f88-87d0-c6d57f51fe1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Make a copy of diabetes\n",
    "diabetes_mean = diabetes.copy(deep=True)\n",
    "\n",
    "# Create mean imputer object\n",
    "mean_imputer = SimpleImputer(strategy='mean') # 'median', 'mode', 'constant'\n",
    "\n",
    "# Impute mean values in the DataFrame diabetes_mean\n",
    "diabetes_mean.iloc[:, :] = mean_imputer.fit_transform(diabetes_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5d6e3-84ce-436e-ad55-2e600e8dd3fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set nrows and ncols to 2\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "nullity = diabetes.Serum_Insulin.isnull()+diabetes.Glucose.isnull()\n",
    "\n",
    "# Create a dictionary of imputations\n",
    "imputations = {'Mean Imputation': diabetes_mean, 'Median Imputation': diabetes_median, \n",
    "               'Most Frequent Imputation': diabetes_mode, 'Constant Imputation': diabetes_constant}\n",
    "\n",
    "# Loop over flattened axes and imputations\n",
    "for ax, df_key in zip(axes.flatten(), imputations):\n",
    "    # Select and also set the title for a DataFrame\n",
    "    imputations[df_key].plot(x='Serum_Insulin', y='Glucose', kind='scatter', \n",
    "                          alpha=0.5, c=nullity, cmap='rainbow', ax=ax, \n",
    "                          colorbar=False, title=df_key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f85af9-2155-4e1a-a2b8-3834bfd9fd40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print prior to interpolation\n",
    "print(airquality[30:40])\n",
    "\n",
    "# Interpolate the NaNs linearly\n",
    "# quadratic, nearest\n",
    "airquality.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Print after interpolation\n",
    "print(airquality[30:40])\n",
    "\n",
    "# Set nrows to 3 and ncols to 1\n",
    "fig, axes = plt.subplots(3, 1, figsize=(30, 20))\n",
    "\n",
    "# Create a dictionary of interpolated DataFrames for looping \n",
    "interpolations = {'Linear Interpolation': linear_interp, 'Quadratic Interpolation': quadratic_interp, \n",
    "                  'Nearest Interpolation': nearest_interp}\n",
    "\n",
    "# Loop over axes and interpolations\n",
    "for ax, df_key in zip(axes, interpolations):\n",
    "    # Select and also set the title for a DataFrame\n",
    "    interpolations[df_key].Ozone.plot(color='red', marker='o', \n",
    "                                 linestyle='dotted', ax=ax)\n",
    "    airquality.Ozone.plot(title=df_key + ' - Ozone', marker='o', ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d29397-71a2-48b4-84c9-9a8a803c7a1f",
   "metadata": {},
   "source": [
    "**fancyimpute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250dc94-2caf-451c-942e-a74e1b614fb3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import KNN from fancyimpute\n",
    "from fancyimpute import KNN\n",
    "\n",
    "# Copy diabetes to diabetes_knn_imputed\n",
    "diabetes_knn_imputed = diabetes.copy(deep=True)\n",
    "\n",
    "# Initialize KNN\n",
    "knn_imputer = KNN()\n",
    "\n",
    "# Impute using fit_tranform on diabetes_knn_imputed\n",
    "diabetes_knn_imputed.iloc[:, :] = knn_imputer.fit_transform(diabetes_knn_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551523e4-eedc-4f38-b658-f8272e2e557a",
   "metadata": {},
   "source": [
    "**ordinal encoding - categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d300c7-67fa-409c-8703-5c171183b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Set col_name to 'ambience'\n",
    "col_name = 'ambience'\n",
    "# Create Ordinal encoder\n",
    "ambience_ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Select non-null values of ambience column in users\n",
    "ambience = users[col_name]\n",
    "ambience_not_null = ambience[ambience.notnull()]\n",
    "\n",
    "# Reshape ambience_not_null to shape (-1, 1)\n",
    "reshaped_vals = ambience_not_null.values.reshape(-1, 1)\n",
    "\n",
    "# Select the non-null values for the column col_name in users and store the encoded values\n",
    "encoded_vals = ambience_ord_enc.fit_transform(reshaped_vals)\n",
    "users.loc[ambience.notnull(), col_name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92cd39-43d5-4f62-a547-e767786e123b",
   "metadata": {},
   "source": [
    "**sklearn KNN imputation** *note `inverse_transform`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586baa6-c09b-4682-a8cb-443cadc44878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN imputer\n",
    "KNN_imputer = KNN()\n",
    "\n",
    "# Impute 'users' DataFrame. It is rounded to get integer values\n",
    "users_KNN_imputed.iloc[:, :] = np.round(KNN_imputer.fit_transform(users))\n",
    "\n",
    "# Loop over the column names in 'users'\n",
    "for col_name in users:\n",
    "    \n",
    "    # Reshape the column data\n",
    "    reshaped = users_KNN_imputed[col_name].values.reshape(-1, 1)\n",
    "    \n",
    "    # Select the column's Encoder and perform inverse transform on 'reshaped'\n",
    "    users_KNN_imputed[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7656b-920a-4a3b-805e-2e1d92e2647c",
   "metadata": {},
   "source": [
    "**statsmodels** imputation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06290-7500-4876-9520-bcafc9cda068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant to X and set X & y values to fit linear model\n",
    "X = sm.add_constant(diabetes_cc.iloc[:, :-1])\n",
    "y = diabetes_cc['Class']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary of lm\n",
    "print('\\nSummary: ', lm.summary())\n",
    "\n",
    "# Print Adjusted R squared score of lm\n",
    "print('\\nAdjusted R-squared score: ', lm.rsquared_adj)\n",
    "\n",
    "# Print the params of lm\n",
    "print('\\nCoefficcients:\\n', lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840dd80-6225-4f9b-9d4f-c44af4047a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_df = {'Mean Imputation': lm_mean.rsquared_adj, \n",
    "             'KNN Imputation': lm_KNN.rsquared_adj, \n",
    "             'MICE Imputation': lm_MICE.rsquared_adj}\n",
    "\n",
    "# Find the maximum Adjusted R-squared score\n",
    "best_imputation = max(rsquared_df, key=rsquared_df.get)\n",
    "\n",
    "print(\"The best imputation technique is: \", best_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609c399-9570-4a46-b93b-a3597afbb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs of imputed DataFrames and the complete case\n",
    "diabetes_cc['Skin_Fold'].plot(kind='kde', c='red', linewidth=3)\n",
    "diabetes_mean_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_knn_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_mice_imputed['Skin_Fold'].plot(kind='kde')\n",
    "\n",
    "# Create labels for the four DataFrames\n",
    "labels = ['Baseline (Complete Case)', 'Mean Imputation', 'KNN Imputation', 'MICE Imputation']\n",
    "plt.legend(labels)\n",
    "\n",
    "# Set the x-label as Skin Fold\n",
    "plt.xlabel('Skin Fold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec394651-aaf0-40be-9117-9f4519bfc3f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Image Data [link](https://app.datacamp.com/learn/courses/image-processing-in-python)\n",
    " - scikit-image, matplotlib\n",
    "   - thresholding\n",
    " - next chapters (not noted yet)\n",
    "   - filters, contrast, transformation, morphology\n",
    "   - image restoration, noise, segmentation, colors\n",
    "   - feature/face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a5b0ba-c5f3-494a-83b8-2a91030aa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to quickly display images during section\n",
    "def show_image(image, title=None, cmap_type='gray'):\n",
    "    plt.imshow(image, cmap=cmap_type)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b555163-496f-4ed5-aae0-321ae9869995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules from skimage\n",
    "from skimage import data, color\n",
    "\n",
    "# Load the rocket image\n",
    "rocket = data.rocket()\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_scaled_rocket = color.rgb2gray(rocket) \n",
    "\n",
    "# Show the original image\n",
    "show_image(rocket, 'Original RGB image')\n",
    "\n",
    "# Show the grayscale image\n",
    "show_image(gray_scaled_rocket, 'Grayscale image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9033a4-4b5c-4feb-b29d-e3ebb2d84fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the image vertically\n",
    "seville_vertical_flip = np.flipud(flipped_seville)\n",
    "\n",
    "# Flip the image horizontally\n",
    "seville_horizontal_flip = np.fliplr(seville_vertical_flip)\n",
    "\n",
    "# Show the resulting image\n",
    "show_image(seville_horizontal_flip, 'Seville')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2c1c0-6c42-4c83-9528-d91d7af5e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the red channel\n",
    "red_channel = image[:, :, 0]\n",
    "\n",
    "# Plot the red histogram with bins in a range of 256\n",
    "plt.hist(red_channel.ravel(), bins=256)\n",
    "\n",
    "# Set title and show\n",
    "plt.title('Red Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16630bdb-c232-4001-9122-14b3a4305855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the otsu threshold function\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# Make the image grayscale using rgb2gray\n",
    "chess_pieces_image_gray = rgb2gray(chess_pieces_image)\n",
    "\n",
    "# Obtain the optimal threshold value with otsu\n",
    "thresh = threshold_otsu(chess_pieces_image_gray)\n",
    "\n",
    "# Apply thresholding to the image\n",
    "binary = chess_pieces_image_gray > thresh\n",
    "\n",
    "# Show the image\n",
    "show_image(binary, 'Binary image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f6a81-8ee8-4eb8-8434-510167e55910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the otsu threshold function\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# Obtain the optimal otsu global thresh value\n",
    "global_thresh = threshold_otsu(page_image)\n",
    "\n",
    "# Obtain the binary image by applying global thresholding\n",
    "binary_global = page_image > global_thresh\n",
    "\n",
    "# Show the binary image obtained\n",
    "show_image(binary_global, 'Global thresholding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d58e6-c8fc-4b95-86b1-4a78b0747d06",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Thresholding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db54e0-13fa-46e6-b966-d57adb049b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the try all function\n",
    "from skimage.filters import try_all_threshold\n",
    "\n",
    "# Import the rgb to gray convertor function \n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Turn the fruits_image to grayscale\n",
    "grayscale = rgb2gray(fruits_image)\n",
    "\n",
    "# Use the try all method on the resulting grayscale image\n",
    "fig, ax = try_all_threshold(grayscale, verbose=False)\n",
    "\n",
    "# Show the resulting plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a8e7e-fc3f-4c58-a8bd-ed6d6a289415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import threshold and gray convertor functions\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Turn the image grayscale\n",
    "gray_tools_image = rgb2gray(tools_image)\n",
    "\n",
    "# Obtain the optimal thresh\n",
    "thresh = threshold_otsu(gray_tools_image)\n",
    "\n",
    "# Obtain the binary image by applying thresholding\n",
    "binary_image = gray_tools_image > thresh\n",
    "\n",
    "# Show the resulting binary image\n",
    "show_image(binary_image, 'Binarized image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8f16c-fa66-49bd-aa19-3f699c6fc78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
