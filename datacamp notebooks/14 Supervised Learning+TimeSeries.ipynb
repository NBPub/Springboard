{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9c6887-7a62-470d-8b89-1930304b738b",
   "metadata": {},
   "source": [
    "- Contents\n",
    "  - [14.1.2](#Supervised-Learning) Supervised Learning\n",
    "    - [Classification](#Classification)\n",
    "    - [Regression](#Regression)\n",
    "    - [Model Evaluation](#Fine-Tuning)\n",
    "    - [Preprocessing and Pipelines](#Preprocessing-and-Pipeliens)\n",
    "  - [14.5.3](#XGBoost) Extreme Gradient Boosting - XGBoost\n",
    "    - [Classification with XGBoost](#XGBoost-Classification)\n",
    "    - [Regression with XGBoost](#XGBoost-Regression)\n",
    "    - [Fine-tuning XGBoost](#XGBoost--Tuning)\n",
    "    - [XGBoost in Pipelines](#XGBoost-Pipelines)\n",
    "  - [14.6.4](#Time-Series-Analysis) Time Series Analysis\n",
    "    - included unit 21 (chapters 4-5) here\n",
    "    - links to section under main header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3f286-f828-4a8f-b0f5-4b0506c16d5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55860d-70b0-481f-827a-54aaaa5a5b07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Classification\n",
    "*sklearn KNN, train-test-split, bias-vs-variance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6314ba8-e143-43c7-ad0d-02a92c3c56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "# Create arrays for the features and the target variable\n",
    "y = churn_df[\"churn\"].values\n",
    "X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n",
    "\n",
    "# Create a KNN classifier with 6 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict the labels for the X_new\n",
    "y_pred = knn.predict(X_new)\n",
    "\n",
    "# Print the predictions for X_new\n",
    "print(\"Predictions: {}\".format(y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffaeef-11d0-48ca-8306-eba783c910ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = churn_df.drop(\"churn\", axis=1).values\n",
    "y = churn_df[\"churn\"].values\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2ab5a-edf5-4b5d-b011-2c34657d5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neighbors\n",
    "neighbors = np.arange(1, 13)\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "for neighbor in neighbors:\n",
    "  \n",
    "\t# Set up a KNN Classifier\n",
    "\tknn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "  \n",
    "\t# Fit the model\n",
    "\tknn.fit(X_train, y_train)\n",
    "  \n",
    "\t# Compute accuracy\n",
    "\ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n",
    "\ttest_accuracies[neighbor] = knn.score(X_test, y_test)\n",
    "print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"KNN: Varying Number of Neighbors\")\n",
    "\n",
    "# Plot training accuracies\n",
    "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
    "\n",
    "# Plot test accuracies\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3f37e-6549-438f-b410-c4e2256dbe66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Regression\n",
    "*basic linear regression, RMSE, cross-validation*\n",
    "\n",
    "*Ridge/Lasso Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9118-7eed-4d64-9d63-046c64d9e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X from the radio column's values\n",
    "X = sales_df.radio.values\n",
    "# Create y from the sales column's values\n",
    "y = sales_df.sales.values\n",
    "# Reshape X\n",
    "X = X.reshape((len(X),1))\n",
    "\n",
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Create the model\n",
    "reg = LinearRegression()\n",
    "# Fit the model to the data\n",
    "reg.fit(X,y)\n",
    "# Make predictions\n",
    "predictions = reg.predict(X)\n",
    "\n",
    "print(predictions[:5])\n",
    "\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(X, y, color=\"blue\")\n",
    "\n",
    "# Create line plot\n",
    "plt.plot(X, predictions, color=\"red\")\n",
    "plt.xlabel(\"Radio Expenditure ($)\")\n",
    "plt.ylabel(\"Sales ($)\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ea7f-5cc9-4148-a2c8-60128536909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute R-squared\n",
    "r_squared = reg.score(X_test, y_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f1d07-afbb-4387-a38c-654f8e004f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 6-fold cross-validation scores\n",
    "cv_scores = cross_val_score(reg,X, y, cv=kf)\n",
    "\n",
    "# Print scores\n",
    "print(cv_scores)\n",
    "\n",
    "# Print the mean\n",
    "print(np.mean(cv_results))\n",
    "\n",
    "# Print the standard deviation\n",
    "print(np.std(cv_results))\n",
    "\n",
    "# Print the 95% confidence interval\n",
    "print(np.quantile(cv_results, [.025, .975]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81920818-babc-4a98-80f0-2f84eaed99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "ridge_scores = []\n",
    "for alpha in alphas:\n",
    "    # Create a Ridge regression model\n",
    "    ridge = Ridge(alpha=alpha) \n",
    "\n",
    "    # Fit the data\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Obtain R-squared\n",
    "    score = ridge.score(X_test,y_test)\n",
    "    ridge_scores.append(score)\n",
    "print(ridge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73820fbe-38f2-4886-8631-d2a8f8f417b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Instantiate a lasso regression model\n",
    "lasso = Lasso(alpha=0.3)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso.fit(X,y)\n",
    "\n",
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "plt.bar(sales_columns, lasso_coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb0ec4-fe32-43a4-9cb6-6dbd264a6c69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Fine Tuning\n",
    "*confusion matrix, ROC curve, ROC-AUC for classification*\n",
    "\n",
    "*GridSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf97a5-5f7a-4a15-becf-7ae1ffc45c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2961d0-7dfe-4be8-a019-f977b569c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_probs = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Diabetes Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88460ef9-9b7d-4a5f-bb9f-774a5c46efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate roc_auc_score\n",
    "print(roc_auc_score(y_test, y_pred_probs))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6bd10-8c52-4a2c-8b63-20625652d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\"alpha\": np.linspace(0.00001, 1, 20)}\n",
    "\n",
    "# Instantiate lasso_cv\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
    "\n",
    "# Fit to the training data\n",
    "lasso_cv.fit(X_train,y_train)\n",
    "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fca19f-43b9-4e0b-80b6-acaa6ef97d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter space\n",
    "params = {\"penalty\": [\"l1\", \"l2\"],\n",
    "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
    "         \"C\": np.linspace(0.1, 1.0, 50),\n",
    "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object\n",
    "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
    "\n",
    "# Fit the data to the model\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c10a86-7504-42c6-ab9d-161caada1e08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Preprocessing and Pipelines\n",
    "*encoding data, imputing missing data, centering/scaling data*\n",
    "\n",
    "*evaluating multiple models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed95bf-272f-470e-a7d1-05ec8dcf9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create music_dummies\n",
    "music_dummies = pd.get_dummies(music_df, drop_first=True)\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# Create X and y\n",
    "X = music_dummies.drop(columns='popularity')\n",
    "y = music_dummies.popularity\n",
    "\n",
    "# Instantiate a ridge model\n",
    "ridge = Ridge(alpha=0.2)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(-scores)\n",
    "print(\"Average RMSE: {}\".format(np.mean(rmse)))\n",
    "print(\"Standard Deviation of the target array: {}\".format(np.std(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fcc94-9762-4dc7-a550-f64877ebb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Instantiate an imputer\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Instantiate a knn model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Build steps for the pipeline\n",
    "steps = [(\"imputer\", imp_mean),\n",
    "        (\"knn\", knn)]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f379ce2-b39e-4b63-8415-92b8b3c4632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create pipeline steps\n",
    "steps = [(\"scaler\", StandardScaler()),\n",
    "         (\"lasso\", Lasso(alpha=0.5))]\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print R-squared\n",
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3e655-7f5b-4abe-b3c5-67b688c1a7ff",
   "metadata": {},
   "source": [
    "**GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36bc7a-770b-45d0-ba9b-ca2b511596df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the steps\n",
    "steps = [(\"scaler\", StandardScaler()),\n",
    "         (\"logreg\", LogisticRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create the parameter space\n",
    "parameters = {\"logreg__C\": np.linspace(0.001, 1.0, 20)}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=21)\n",
    "\n",
    "# Instantiate the grid search object\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit to the training data\n",
    "cv.fit(X_train, y_train)\n",
    "print(cv.best_score_, \"\\n\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe83cc6-1ae1-427e-8c97-eff73a61bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\n",
    "results = []\n",
    "\n",
    "# Loop through the models' values\n",
    "for model in models.values():\n",
    "    kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n",
    "\n",
    "    # Append the results\n",
    "    results.append(cv_scores)\n",
    "\n",
    "# Create a box plot of the results\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa980f-8f9e-465d-9762-6e3d760b5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, but evaluate RMSE on test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for name, model in models.items():\n",
    "  \n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_scaled,y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the test_rmse\n",
    "    test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"{} Test Set RMSE: {}\".format(name, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b6af6-20e6-4ec7-b91f-67514aaef582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models dictionary\n",
    "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\n",
    "results = []\n",
    "\n",
    "# Loop through the models' values\n",
    "for model in models.values():\n",
    "  \n",
    "    # Instantiate a KFold object\n",
    "    kf = KFold(n_splits=6, random_state=12, shuffle=True)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
    "    results.append(cv_results)\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a9fb4-3749-4851-92d6-25d37737eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steps\n",
    "steps = [(\"imp_mean\", SimpleImputer()), \n",
    "         (\"scaler\", StandardScaler()), \n",
    "         (\"logreg\", LogisticRegression())]\n",
    "\n",
    "# Set up pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "params = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n",
    "         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "tuning = GridSearchCV(pipeline, param_grid=params)\n",
    "tuning.fit(X_train, y_train)\n",
    "y_pred = tuning.predict(X_test)\n",
    "\n",
    "# Compute and print performance\n",
    "print(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eeabe-f7f6-4f2f-908b-3ef9bb3da78c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43187e08-2920-470d-8bde-15ae3b2f53a0",
   "metadata": {},
   "source": [
    "**Boosting**<br>\n",
    "*boosting is a meta-algorithm used to reduce a signle-learners variance and to convert many weak learners into an arbitrarily strong learner*\n",
    " - weak: model performing slightly better than chance\n",
    " - strong: model with good performance for supervised learning problem\n",
    " \n",
    "weigh multiple weak learner's predictions according to performance, combine weighted result to create a \"strong\" learner\n",
    "\n",
    "**XGBoost package**<br>\n",
    " - convert data into `DMatrix`, optimized datastructure for XGBoost\n",
    " - cross validation method provided `cv(dtrain=, params=, nfold= ,...)`\n",
    "   - variety of scoring metrix available\n",
    " - built in plotting functions: `plot_tree`, `plot_importance`\n",
    "\n",
    "**when to use XGBoost**<br>\n",
    " - large dataset, available training samples\n",
    "   - samples much greater than features is better\n",
    " - can handle mix of categorical and numeric features\n",
    " \n",
    "**do NOT use for**<br>\n",
    " - image recognition, computer vision, NLP\n",
    "   - *deep learning is better*\n",
    " - small dataset size and/or feature > samples\n",
    "\n",
    "**Not covered in course**\n",
    " - ranking/recommendation problems --> modify loss function\n",
    " - Bayesian optimization for hyper parameter tuning\n",
    " - combining XG boost with  other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d5ebb-2f10-4b64-b5fe-a8b2ff41ae06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474f359-bdda-44e9-821e-a733fea8a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DescisionTreeClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551847b-bb4d-4ca6-8650-f6bcf95109f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train,X_test,y_train,y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645403c8-f13b-46a9-aed8-03c54c06da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descision tree to compare to ???\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ace29b-2128-44b4-ae01-6f9a41e3126d",
   "metadata": {},
   "source": [
    "**DMatrix, Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30946266-3e64-4a4c-b46c-eed8ba5d2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                    nfold=3, num_boost_round=5, \n",
    "                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "\n",
    "# with AUC as metric\n",
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                    nfold=3, num_boost_round=5, \n",
    "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018564d-08a7-4430-b5eb-0caca48b98c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### XGBoost Regression\n",
    " - **objective function:** qunatifies error, minimize for model selection. various functions available in **XGBoost**\n",
    "   - regression\n",
    "     - reg:linear\n",
    "     - various **base learners** available as well. compared in code example\n",
    "       - default: `booster='gbtree'`\n",
    "       - ex: `booster='gblinear'`\n",
    "     - loss functions, **regularization** \n",
    "   - classification\n",
    "     - reg:logistic *label prediction only, no probability*\n",
    "     - binary:logistic *probability included*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6a1f8-3df9-4fb8-aaa7-58938d97d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree vs linear base learner\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457d834-8332-4c49-bde9-15130a84b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5595c-8ca5-482c-a546-f672a7099136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation, RMSE, MAE. set cv scoring metric\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "# Perform cross-validation: cv_results, change rmse to mae\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "# Extract and print final round boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
    "# Extract and print final round boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87570096-7ed6-47c4-b642-0880a115cf3e",
   "metadata": {},
   "source": [
    "**Regularization**\n",
    "- manual grid search example\n",
    "- note xgb has `plot_tree` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9dae67-e582-442a-a17f-c126ca0be776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "reg_params = [1, 10, 100]\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg  \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)  \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2e31d-6efe-48bd-9f97-47ca10757225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14f076-606a-47e7-a5af-ca42037a1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d27255-aa1f-42f8-aae1-e9ee5d719778",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### XGBoost  Tuning\n",
    " - early stopping with cross-validation\n",
    " - examples\n",
    "   - learning rate, `eta`\n",
    "   - `max_depth`\n",
    "   - max samples AKA `colsample_bytree`\n",
    " - grid search, random search\n",
    "   - beware of increasing time with too many grid search parameters\n",
    "   - random search suffers from chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ee234-fa78-4284-9545-a5eb0350847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a732dfe-61e5-40b2-b948-e6d118d144d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=10, early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d328bb-f4e9-48d2-8758-29b431c9143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952e3a2-2751-48e3-9ccb-b2ed8d6697cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6bcd75-c073-4bad-8f7c-5d7d6f08fcca",
   "metadata": {},
   "source": [
    "**Grid Search, Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2e575-ef28-4ef4-84d9-ef03f7329cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71561bb5-3ed3-4e6e-93da-582fddfea136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bec658-0245-4dcb-9159-14a4bcf00ac3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### XGBoost Pipelines\n",
    " - incorporating xgboost model with sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9fb1b2-2f9c-4d5a-830b-15d5fa857360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and encode data\n",
    "\n",
    "# Import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict(\"records\")\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149efdf-172d-4b0c-b4c0-eb7313a3a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd5513-4fcc-48e7-afcc-7c6d6f3659b0",
   "metadata": {},
   "source": [
    "*put it all together*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa7f6d-037c-4813-b1bc-095c243da67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c9db6-9e63-4328-9355-b18fef935dbf",
   "metadata": {},
   "source": [
    "*another example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f465f-d97c-4c3a-bf8a-ec93cbfba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(categorical_columns, SimpleImputer(strategy='most_frequent'))],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c7183-41dc-440b-b493-b83c6a06597d",
   "metadata": {},
   "source": [
    "*final execution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a3daa-bc67-4121-a41d-0e494c79c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "    'clf__max_depth': np.arange(3,10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9425cce-8eb9-494d-b473-ac34c79b915b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series Analysis\n",
    "chapters 1-5\n",
    " - [Time Series Data, Correlation, Autocorrelation](#Chapter-One)\n",
    " - [Autocorrelation Function](#Chapter-Two---Autocorrelation-Function)\n",
    " - [Autoregressive (AR) Models](#Chapter-Three---Autoregressive-(AR)-Models)\n",
    " - [Moving Average (MA) Models, ARMA](#Chapter-Four---Moving-Average-(MA)-Models)\n",
    " - [Putting it all together](#Chapter-Five---Example)\n",
    " \n",
    "advanced topics recommended after chapter 5\n",
    " - GARCH, Nonlinear, Multivariate Time Series, Regime Switching, State Space models\n",
    " - Kalman Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5109e-6598-4943-9509-073f011a1d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Chapter One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05706f5-8e58-4c80-99a4-a00b7d16e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diet is some search trends data frame, convert index to time\n",
    "# Import pandas and plotting modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the date index to datetime\n",
    "diet.index = pd.to_datetime(diet.index)\n",
    "\n",
    "# Slice the dataset to keep only 2012\n",
    "diet2012 = diet[diet.index.year==2012]\n",
    "\n",
    "# Plot the entire time series diet and show gridlines\n",
    "diet.plot(grid=True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2012 data\n",
    "diet2012.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3de41-a3d9-41dd-a998-485bbf2b6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging time series with different dates\n",
    "# make set of two indices, take difference, \n",
    "# then do df1.join(df2, how='inner')\n",
    "\n",
    "# Compute percent change using pct_change()\n",
    "returns = stocks_and_bonds.pct_change()\n",
    "\n",
    "# Compute correlation using corr()\n",
    "correlation = returns.SP500.corr(returns.US10Y)\n",
    "print(\"Correlation of stocks and interest rates: \", correlation)\n",
    "\n",
    "# Make scatter plot\n",
    "plt.scatter(returns.SP500, returns.US10Y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# compare correlation before/after percent change\n",
    "# Compute correlation of levels\n",
    "correlation1 = levels.DJI.corr(levels.UFO)\n",
    "print(\"Correlation of levels: \", correlation1)\n",
    "\n",
    "# Compute correlation of percent changes\n",
    "changes = levels.pct_change()\n",
    "correlation2 = changes.DJI.corr(changes.UFO)\n",
    "print(\"Correlation of changes: \", correlation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e7ed9-92eb-44b3-b1d9-fd32a3ceaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression, statsmodels\n",
    "\n",
    "# Import the statsmodels module\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Compute correlation of x and y\n",
    "correlation = x.corr(y)\n",
    "print(\"The correlation between x and y is %4.2f\" %(correlation))\n",
    "\n",
    "# Convert the Series x to a DataFrame and name the column x\n",
    "dfx = pd.DataFrame(x, columns=['x'])\n",
    "\n",
    "# Add a constant to the DataFrame dfx\n",
    "dfx1 = sm.add_constant(dfx)\n",
    "\n",
    "# Regress y on dfx1\n",
    "result = sm.OLS(y, dfx1).fit()\n",
    "\n",
    "# Print out the results and look at the relationship between R-squared and the correlation above\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c793a39-8d2a-44f2-a130-0fe1464687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation, time series with itself\n",
    "# for this dataset, try resampling to week/year\n",
    "\n",
    "# Convert the daily data to weekly data\n",
    "MSFT = MSFT.resample(rule='W').last()\n",
    "\n",
    "# Compute the percentage change of prices\n",
    "returns = MSFT.pct_change()\n",
    "\n",
    "# Compute and print the autocorrelation of returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly returns is %4.2f\" %(autocorrelation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115fd1a-0e80-4e26-b873-6fc5850f4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the daily change in interest rates \n",
    "daily_diff = daily_rates.diff()\n",
    "\n",
    "# Compute and print the autocorrelation of daily changes\n",
    "autocorrelation_daily = daily_diff['US10Y'].autocorr()\n",
    "print(\"The autocorrelation of daily interest rate changes is %4.2f\" %(autocorrelation_daily))\n",
    "\n",
    "# Convert the daily data to annual data\n",
    "yearly_rates = daily_rates.resample(rule='A').last()\n",
    "\n",
    "# Repeat above for annual data\n",
    "yearly_diff = yearly_rates.diff()\n",
    "autocorrelation_yearly = yearly_diff['US10Y'].autocorr()\n",
    "print(\"The autocorrelation of annual interest rate changes is %4.2f\" %(autocorrelation_yearly))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0841360-6b18-4245-93a1-8395d1b3427f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Chapter Two - Autocorrelation Function\n",
    " - statsmodels `plot_acf` [docs](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2b5e2-2eb8-4b89-a748-4d6d2e73ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0427e00-feb4-4724-b1fe-d790f6378b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the acf module and the plot_acf module from statsmodels\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Compute the acf array of HRB\n",
    "acf_array = acf(HRB)\n",
    "print(acf_array)\n",
    "\n",
    "# Plot the acf function\n",
    "plot_acf(HRB,alpha=1) # alpha=1 suppresses confidence interval\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa79214-ffc4-4eea-a324-06f2246b1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels and sqrt from math\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from math import sqrt\n",
    "\n",
    "# Compute and print the autocorrelation of MSFT weekly returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly MSFT returns is %4.2f\" %(autocorrelation))\n",
    "\n",
    "# Find the number of observations by taking the length of the returns DataFrame\n",
    "nobs = len(returns)\n",
    "\n",
    "# Compute the approximate confidence interval\n",
    "conf = 1.96/sqrt(nobs)\n",
    "print(\"The approximate confidence interval is +/- %4.2f\" %(conf))\n",
    "\n",
    "# Plot the autocorrelation function with 95% confidence intervals and 20 lags using plot_acf\n",
    "plot_acf(returns, alpha=0.05, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d46f0-cfb5-4cf9-bf73-61c724cee888",
   "metadata": {},
   "source": [
    "**White Noise** series with\n",
    " - constant mean\n",
    " - constant variance\n",
    " - zero autocorrelation at all lags\n",
    " - *Gaussian* if distributed normally\n",
    " \n",
    "**Random Walk**\n",
    " - can't forecast, best to use today to predict tomorrow\n",
    " - Dickey-Fuller test for random walk [wiki](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test)\n",
    " - statsmodels has `adfuller` to perfrom Augmented Dickey-Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29ef1a-2b0a-4680-85e0-5a5dca2736d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec2164-5ee5-44c2-a8c4-7c3cbdbbbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate white noise returns\n",
    "returns = np.random.normal(loc=0.02, scale=0.05, size=1000)\n",
    "\n",
    "# Print out the mean and standard deviation of returns\n",
    "mean = np.mean(returns)\n",
    "std = np.std(returns)\n",
    "print(\"The mean is %5.3f and the standard deviation is %5.3f\" %(mean,std))\n",
    "\n",
    "# Plot returns series\n",
    "plt.plot(returns)\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation function of white noise returns\n",
    "plot_acf(returns, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39db519-f72c-4a9e-b8df-18c5ee681084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 random steps with mean=0 and standard deviation=1\n",
    "steps = np.random.normal(loc=0, scale=1.0, size=500)\n",
    "\n",
    "# Set first element to 0 so that the first price will be the starting stock price\n",
    "steps[0]=0\n",
    "\n",
    "# Simulate stock prices, P with a starting price of 100\n",
    "P = 100 + np.cumsum(steps)\n",
    "\n",
    "# Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80838c8d-0ada-4de9-aae1-e4a97d80ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Walk with drift\n",
    "# Generate 500 random steps\n",
    "steps = np.random.normal(loc=0.001, scale=0.01, size=500) + 1\n",
    "\n",
    "# Set first element to 1\n",
    "steps[0]=1\n",
    "\n",
    "# Simulate the stock price, P, by taking the cumulative product\n",
    "P = 100 * np.cumprod(steps)\n",
    "\n",
    "# Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk with Drift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030b60c-e8da-423c-9a86-32d6122a6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is amazon closing price a random walk?\n",
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run the ADF test on the price series and print out the results\n",
    "results = adfuller(AMZN['Adj Close'])\n",
    "print(results)\n",
    "\n",
    "# Just print out the p-value\n",
    "print('The p-value of the test on prices is: ' + str(results[1]))\n",
    "\n",
    "# cannot reject hypothesis, p value high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee8a4e-cf7f-4470-99d9-926a16fe4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of AMZN returns (percent change)\n",
    "AMZN_ret = AMZN.pct_change()\n",
    "\n",
    "# Eliminate the NaN in the first row of returns\n",
    "AMZN_ret = AMZN_ret.dropna()\n",
    "\n",
    "# Run the ADF test on the return series and print out the p-value\n",
    "results = adfuller(AMZN_ret)\n",
    "print('The p-value of the test on returns is: ' + str(results[1]))\n",
    "\n",
    "# p-value very small, can easily reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ba68a-3585-4bea-b9db-5a932e55ee93",
   "metadata": {},
   "source": [
    "**Stationarity**\n",
    " - *strong* distribution of data is time-invariant\n",
    " - *weak* mean, variance, and autocorrelation are time-invariant\n",
    "   - random walk is non-stationary, variance grows with time\n",
    "   - seasonal series are non-stationary, mean varies with time of year\n",
    " - can transfrom nonstationary series to stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf507cf-43fb-47b1-9e8b-390d542a26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Seasonally adjust quarterly earnings\n",
    "HRBsa = HRB.diff(periods=4)\n",
    "\n",
    "# Print the first 10 rows of the seasonally adjusted series\n",
    "print(HRBsa.iloc[:10,:])\n",
    "\n",
    "# Drop the NaN data in the first four rows\n",
    "HRBsa = HRBsa.dropna()\n",
    "\n",
    "# Plot the autocorrelation function of the seasonally adjusted series\n",
    "plot_acf(HRBsa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f154b-a423-4c2a-bc04-ddd299d8014e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Chapter Three - Autoregressive (AR) Models\n",
    " - [Autoregressive Model wiki](https://en.wikipedia.org/wiki/Autoregressive_model), various orders AR(1), AR(2). . .\n",
    "\n",
    "> The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term). . . Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure\n",
    " - statsmodels `ArmaProcess`\n",
    " - statsmodels `ARIMA` over `ARMA`, latter is deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92a135-1631-4010-91ac-46746526723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20666553-1130-4290-8e28-8a739cb9ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note sign switch of AR parameter\n",
    "\n",
    "# Plot 1: AR parameter = +0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1, -0.9])\n",
    "ma1 = np.array([1])\n",
    "AR_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = AR_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: AR parameter = -0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1, 0.9])\n",
    "ma2 = np.array([1])\n",
    "AR_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = AR_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79219f7b-18f8-448a-a5c3-16dc72274f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot 1: AR parameter = +0.9\n",
    "plot_acf(simulated_data_1, alpha=1, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff00624-de51-4278-90e3-95e5d40523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARIMA module from statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit an AR(1) model to the first simulated data\n",
    "mod = ARIMA(simulated_data_1, order=(1,0,0))\n",
    "res = mod.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res.summary())\n",
    "\n",
    "# Print out the estimate for phi\n",
    "print(\"When the true phi=0.9, the estimate of phi is:\")\n",
    "print(res.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb1b0e-076d-45d9-aad7-6249d7d258e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the forecast\n",
    "fig, ax = plt.subplots()\n",
    "simulated_data_1.loc[950:].plot(ax=ax)\n",
    "plot_predict(res, start=1000, end=1010, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4f9ef-c811-4158-b594-d7985368381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast interst rates using an AR(1) model\n",
    "mod = ARIMA(interest_rate_data, order=(1,0,0))\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the data and the forecast\n",
    "fig, ax = plt.subplots()\n",
    "interest_rate_data.plot(ax=ax)\n",
    "plot_predict(res, start=0, end='2027', alpha=None, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af6e02-ab87-44c6-8536-6a5556e70cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast interst rates using an AR(1) model\n",
    "mod = ARIMA(interest_rate_data, order=(1,0,0))\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the data and the forecast\n",
    "fig, ax = plt.subplots()\n",
    "interest_rate_data.plot(ax=ax)\n",
    "plot_predict(res, start=0, end='2027', alpha=None, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7e5a2-2d0d-4a5e-91ba-c95d17fd9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot the interest rate series and the simulated random walk series side-by-side\n",
    "fig, axes = plt.subplots(2,1)\n",
    "\n",
    "# Plot the autocorrelation of the interest rate series in the top plot\n",
    "fig = plot_acf(interest_rate_data, alpha=1, lags=12, ax=axes[0])\n",
    "\n",
    "# Plot the autocorrelation of the simulated random walk series in the bottom plot\n",
    "fig = plot_acf(simulated_data, alpha=1, lags=12, ax=axes[1])\n",
    "\n",
    "# Label axes\n",
    "axes[0].set_title(\"Interest Rate Data\")\n",
    "axes[1].set_title(\"Simulated Random Walk Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeec5b5-a984-4123-99ba-3a10e2fff3ac",
   "metadata": {},
   "source": [
    "**Choosing the Right Order - Information Criteria**\n",
    " - higher orders will fit better, AIC and BIC score goodness of fit / number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3612-5895-438c-94b7-8a75017b528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules for simulating data and for plotting the PACF\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Simulate AR(1) with phi=+0.6\n",
    "ma = np.array([1])\n",
    "ar = np.array([1, -0.6])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data_1 = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot PACF for AR(1)\n",
    "plot_pacf(simulated_data_1, lags=20)\n",
    "plt.show()\n",
    "\n",
    "# Simulate AR(2) with phi1=+0.6, phi2=+0.3\n",
    "ma = np.array([1])\n",
    "ar = np.array([1, -0.6, -0.3])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data_2 = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot PACF for AR(2)\n",
    "plot_pacf(simulated_data_2, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb58b8-12aa-4322-ae0e-510544cbad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for estimating an ARIMA model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit the data to an AR(p) for p = 0,...,6 , and save the BIC\n",
    "BIC = np.zeros(7)\n",
    "for p in range(7):\n",
    "    mod = ARIMA(simulated_data_2, order=(p,0,0))\n",
    "    res = mod.fit()\n",
    "# Save BIC for AR(p)    \n",
    "    BIC[p] = res.bic\n",
    "    \n",
    "# Plot the BIC as a function of p\n",
    "plt.plot(range(1,7), BIC[1:7], marker='o')\n",
    "plt.xlabel('Order of AR Model')\n",
    "plt.ylabel('Bayesian Information Criterion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c558ad-d41c-46fa-a73d-a236d3c0db5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Chapter Four - Moving Average (MA) Models\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Moving-average_model)\n",
    "\n",
    "> In time series analysis, the moving-average model (MA model), also known as moving-average process, is a common approach for modeling univariate time series.[1][2] The moving-average model specifies that the output variable is cross-correlated with a non-identical to itself random-variable.\n",
    "<br><br>Together with the autoregressive (AR) model, the moving-average model is a special case and key component of the more general ARMA and ARIMA models of time series,[3] which have a more complicated stochastic structure. Contrary to the AR model, the finite MA model is always stationary.\n",
    "\n",
    "As in the last chapter, when inputting the coefficients, you must include the zero-lag coefficient of 1, but unlike the last chapter on AR models, the sign of the MA coefficients is what we would expect. \n",
    "\n",
    "Create an instance of the ARIMA class. . .the order (p,d,q) of the model (in this case, for an MA(1)), is order=(0,0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d34e93-e8bb-49ec-b070-058bc7a59326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1])\n",
    "ma1 = np.array([1, -0.9])\n",
    "MA_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = MA_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: MA parameter = +0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1])\n",
    "ma2 = np.array([1, 0.9])\n",
    "MA_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = MA_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d1314-5432-46df-a51b-44dd8673d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(simulated_data_1, lags=20)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(simulated_data_2, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95c694-432b-4d47-8ead-b1b22b29ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARIMA module from statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit an MA(1) model to the first simulated data\n",
    "mod = ARIMA(simulated_data_1, order=(0,0,1))\n",
    "res = mod.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res.summary())\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "print(\"When the true theta=-0.9, the estimate of theta is:\")\n",
    "print(res.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99e248-8ef7-4a37-9692-5df017ecbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARIMA and plot_predict from statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "\n",
    "# Forecast the first MA(1) model\n",
    "mod = ARIMA(simulated_data_1, order=(0,0,1))\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the data and the forecast\n",
    "fig, ax = plt.subplots()\n",
    "simulated_data_1.loc[950:].plot(ax=ax)\n",
    "plot_predict(res, start=1000, end=1010, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00127c4f-462c-4858-bee5-adaefb113f7f",
   "metadata": {},
   "source": [
    "**ARMA Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf74f4-fd2c-4542-93df-622ebec415c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing rows using difference of sets\n",
    "set_missing = set(range(391)) - set(intraday.index)\n",
    "\n",
    "# Print the difference\n",
    "print(\"Missing rows: \", set_missing)\n",
    "\n",
    "# Fill in the missing rows\n",
    "intraday = intraday.reindex(range(391), method='ffill')\n",
    "\n",
    "intraday.index = pd.date_range(start='2017-09-01 9:30', end='2017-09-01 16:00', freq='1min')\n",
    "\n",
    "# Plot the intraday time series\n",
    "intraday.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bff7a-db97-4fd5-812d-a17b8c860f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_acf and ARIMA modules from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Compute returns from prices and drop the NaN\n",
    "returns = intraday.pct_change()\n",
    "returns = returns.dropna()\n",
    "\n",
    "# Plot ACF of returns with lags up to 60 minutes\n",
    "plot_acf(returns, lags=60)\n",
    "plt.show()\n",
    "\n",
    "# Fit the data to an MA(1) model\n",
    "mod = ARIMA(returns, order=(0,0,1))\n",
    "res = mod.fit()\n",
    "print(res.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c2ded-3253-4152-9988-86eefc2ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules for simulating data and plotting the ACF\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Build a list MA parameters\n",
    "ma = [0.8**i for i in range(30)]\n",
    "\n",
    "# Simulate the MA(30) model\n",
    "ar = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(simulated_data, lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c9df5-e3f0-43fc-8c58-46b8e3427cfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Chapter Five - Example\n",
    " - **CoIntegration** - two series may be random walks, but their combination might not be (is forecastable)\n",
    "   - various economic examples listed (related subsidies, competing companies)\n",
    " - statsmodels `coint` function\n",
    " - Test for cointegration, of series `P` and `Q`, in two steps:\n",
    "   - 1: Regress both series to find slope, `c`\n",
    "   - 2: Run augmented Dickey-Fuller test on `P + cQ` to see if resulting combination is random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68629c5-0a29-4244-90d9-2d2ce622716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heating Oil and Natural Gas prices, first 7.25 is a unit conversion\n",
    "\n",
    "# Plot the prices separately\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(7.25*HO, label='Heating Oil')\n",
    "plt.plot(NG, label='Natural Gas')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "\n",
    "# Plot the spread\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(7.25*HO-NG, label='Spread')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.axhline(y=0, linestyle='--', color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a608f20-83cc-4ea6-9029-01fa9105c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Compute the ADF for HO and NG\n",
    "result_HO = adfuller(HO['Close'])\n",
    "print(\"The p-value for the ADF test on HO is \", result_HO[1])\n",
    "result_NG = adfuller(NG['Close'])\n",
    "print(\"The p-value for the ADF test on NG is \", result_NG[1])\n",
    "\n",
    "# Compute the ADF of the spread\n",
    "result_spread = adfuller(7.25 * HO['Close'] - NG['Close'])\n",
    "print(\"The p-value for the ADF test on the spread is \", result_spread[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14267911-be0b-4e46-8c64-107492f1a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels module for regression and the adfuller function\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regress BTC on ETH\n",
    "ETH = sm.add_constant(ETH)\n",
    "result = sm.OLS(BTC,ETH).fit()\n",
    "\n",
    "# Compute ADF\n",
    "b = result.params[1]\n",
    "adf_stats = adfuller(BTC['Price'] - b*ETH['Price'])\n",
    "print(\"The p-value for the ADF test is \", adf_stats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba810d-02eb-49e1-b4fe-1a49dae6c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller function from the statsmodels module\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Convert the index to a datetime object\n",
    "temp_NY.index = pd.to_datetime(temp_NY.index, format='%Y')\n",
    "\n",
    "# Plot average temperatures\n",
    "temp_NY.plot()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print ADF p-value\n",
    "result = adfuller(temp_NY['TAVG'])\n",
    "print(\"The p-value for the ADF test is \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532583a-bf82-4c28-b238-6f9ad11995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules for plotting the sample ACF and PACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Take first difference of the temperature Series\n",
    "chg_temp = temp_NY.diff()\n",
    "chg_temp = chg_temp.dropna()\n",
    "\n",
    "# Plot the ACF and PACF on the same page\n",
    "fig, axes = plt.subplots(2,1)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(chg_temp, lags=20, ax=axes[0])\n",
    "\n",
    "# Plot the PACF\n",
    "plot_pacf(chg_temp, lags=20, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c18141-c18e-406e-945c-22e625afca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for estimating an ARIMA model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit the data to an AR(1) model and print AIC:\n",
    "mod_ar1 = ARIMA(chg_temp, order=(1, 0, 0))\n",
    "res_ar1 = mod_ar1.fit()\n",
    "print(\"The AIC for an AR(1) is: \", res_ar1.aic)\n",
    "\n",
    "# Fit the data to an AR(2) model and print AIC:\n",
    "mod_ar2 = ARIMA(chg_temp, order=(2, 0, 0))\n",
    "res_ar2 = mod_ar2.fit()\n",
    "print(\"The AIC for an AR(2) is: \", res_ar2.aic)\n",
    "\n",
    "# Fit the data to an ARMA(1,1) model and print AIC:\n",
    "mod_arma11 = ARIMA(chg_temp, order=(1, 1, 0))\n",
    "res_arma11 = mod_arma11.fit()\n",
    "print(\"The AIC for an ARMA(1,1) is: \", res_arma11.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802162e-6513-450a-9474-a1a6e8f99205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARIMA module from statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "\n",
    "# Forecast temperatures using an ARIMA(1,1,1) model\n",
    "mod = ARIMA(temp_NY, trend='t', order=(1,1,1))\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the original series and the forecasted series\n",
    "fig, ax = plt.subplots()\n",
    "temp_NY.plot(ax=ax)\n",
    "plot_predict(res, start='1872', end='2046', ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
